{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dx3P_w2gU-DY"
   },
   "source": [
    "# <u>Muestreo, Normalización y Estandarización</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importación de librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FKJsQrxuU-DZ"
   },
   "outputs": [],
   "source": [
    "# Importar librerias necesarias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.stats as st\n",
    "import statsmodels.api as sm \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVeZWllkU-Dh"
   },
   "outputs": [],
   "source": [
    "# Importar datos en formato xls.\n",
    "data_hipotecas = pd.read_excel('data/hipotecas.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hallazgos iniciales\n",
    "print('dimensiones',data_hipotecas.shape)\n",
    "print('Nro de tiendas:', len(np.unique(data_hipotecas.TIENDA)))\n",
    "print('Nro de vendedores:', len(np.unique(data_hipotecas.ID_VENDEDOR)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hipotecas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnicas de selección de elementos por muestreo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Muestreo aleatorio simple:\n",
    "\n",
    "De acuerdo con el principio de igual probabilidad, se seleccionan aleatoriamente n muestras de la población.\n",
    "\n",
    "Escenario aplicable: todos los individuos de la muestra son igualmente probables (uniformes).\n",
    "\n",
    "Según los escenarios aplicables, el muestreo aleatorio se divide en muestreo con reemplazo y muestreo sin reemplazo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kAzFNHsrWMV1"
   },
   "outputs": [],
   "source": [
    "# Muestreo aleatorio simple por cantidad\n",
    "print('Data completa=',data_hipotecas.shape)\n",
    "print('Data MAS=',data_hipotecas.sample(n=50, replace=False).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = data_hipotecas.sample(n=50, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample['TIENDA'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hipotecas['TIENDA'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample['TIENDA'].value_counts(normalize = True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hipotecas['TIENDA'].value_counts(normalize = True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestreo aleatorio simple por porcentaje\n",
    "print('Data completa=',data_hipotecas.shape)\n",
    "print('Data MAS=',data_hipotecas.sample(frac = 0.1, replace=False).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample_frac = data_hipotecas.sample(frac = 0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample_frac['TIENDA'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hipotecas['TIENDA'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample_frac['TIENDA'].value_counts(normalize = True).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hipotecas['TIENDA'].value_counts(normalize = True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Muestreo estratificado:\n",
    "\n",
    "Divida todas las muestras individuales en varias categorías de acuerdo con ciertas características y luego aplique un muestreo aleatorio o un muestreo equidistante de cada categoría para seleccionar individuos para formar una muestra.\n",
    "\n",
    "Ventajas: Puede reducir significativamente los errores de muestreo y facilitar la investigación por separado para diferentes tipos de muestras de datos.\n",
    "\n",
    "Escenarios aplicables: datos con características como atributos y etiquetas de lógica de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestreo Estratificado por cantidad\n",
    "df_estrato = data_hipotecas.groupby('TIENDA', group_keys=False).apply(lambda x: x.sample(n = 200, replace = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estrato['TIENDA'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestreo Estratificado por proporción\n",
    "df_estrato2 = data_hipotecas.groupby('TIENDA', group_keys=False).apply(lambda x: x.sample(frac = 0.1, replace = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_estrato2['TIENDA'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l2pg_wVfU-EQ"
   },
   "source": [
    "## Pruebas de Normalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DNT3fy_HU-ER"
   },
   "outputs": [],
   "source": [
    "# Importar datos en formato xls.\n",
    "churners_data = pd.read_excel('data/Churners.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lQIn1D4GXbl-"
   },
   "outputs": [],
   "source": [
    "churners_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAhM-0WYXdtr"
   },
   "outputs": [],
   "source": [
    "churners_data.groupby(['CHURN'])['ID'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WY4uZOXMU-EY"
   },
   "outputs": [],
   "source": [
    "churners_data_vol=churners_data[churners_data['CHURN']=='Voluntario']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qx_l8oQOXgS7"
   },
   "outputs": [],
   "source": [
    "churners_data_vol.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionamos la variable de interés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mlAW7FxrU-Ec"
   },
   "outputs": [],
   "source": [
    "churners_ing=churners_data_vol['INGRESO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aWqzs9ImXizV"
   },
   "outputs": [],
   "source": [
    "churners_ing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Gráfico de Histograma\n",
    "Un gráfico simple y de uso común para verificar rápidamente la distribución de una muestra de datos es el histograma. En el histograma, los datos se dividen en un número predeterminado de grupos llamados bins. Luego, los datos se clasifican en cada contenedor y se retiene el recuento del número de observaciones en cada contenedor.\n",
    "\n",
    "El gráfico muestra los contenedores en el eje x manteniendo su relación ordinal, y el recuento en cada contenedor en el eje y. Una muestra de datos tiene una distribución gaussiana del diagrama del histograma, que muestra la forma familiar de campana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis gráfico\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "plt.hist(churners_ing,bins=50,color='blue')\n",
    "plt.title('Distribución de ingresos',fontsize=20)\n",
    "plt.ylabel('Frecuencia',fontsize=20)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Gráfico de cuantiles-cuantiles\n",
    "Otro gráfico popular para verificar la distribución de una muestra de datos es el gráfico de cuantiles-cuantiles, gráfico QQ o gráfico QQ para abreviar. Esta gráfica genera su propia muestra de la distribución idealizada con la que estamos comparando, en este caso la distribución gaussiana. Las muestras idealizadas se dividen en grupos (por ejemplo, 5), llamados cuantiles. Cada punto de datos de la muestra se empareja con un miembro similar de la distribución idealizada en la misma distribución acumulativa.\n",
    "\n",
    "Los puntos resultantes se trazan como un diagrama de dispersión con el valor idealizado en el eje xy la muestra de datos en el eje y. Una línea de puntos en un ángulo de 45 grados desde la parte inferior izquierda del gráfico hasta la parte superior derecha mostrará una coincidencia perfecta para la distribución. A menudo, se traza una línea en la trama para ayudar a aclarar esta expectativa. Las desviaciones de los puntos de la línea muestran una desviación de la distribución esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.qqplot(churners_ing, line='s') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruebas de normalidad estadística\n",
    "Hay muchas pruebas estadísticas que podemos utilizar para cuantificar si una muestra de datos parece extraída de una distribución gaussiana. Cada prueba hace diferentes suposiciones y considera diferentes aspectos de los datos.\n",
    "\n",
    "En esta sección, analizaremos 3 pruebas de uso común que puede aplicar a sus propias muestras de datos.\n",
    "\n",
    "Interpretación de una prueba\n",
    "Cada prueba devolverá al menos dos cosas:\n",
    "\n",
    "* **Estadística :** una cantidad calculada por la prueba que se puede interpretar en el contexto de la prueba comparándola con los valores críticos de la distribución de la estadística de prueba.\n",
    "* **valor p :** se utiliza para interpretar la prueba, en este caso si la muestra se extrajo de una distribución gaussiana.\n",
    "Las pruebas suponen que la muestra se extrajo de una distribución gaussiana. Técnicamente, esto se llama hipótesis nula o H0. Se elige un nivel de umbral llamado alfa, típicamente 5% (o 0.05), que se usa para interpretar el valor p.\n",
    "\n",
    "En la implementación de SciPy de estas pruebas, puede interpretar el valor p de la siguiente manera.\n",
    "* p <= alfa : rechazar H0, no es normal.\n",
    "* p> alpha : falla al rechazar H0, normal.\n",
    "\n",
    "Un resultado superior al 5% no significa que la hipótesis nula sea cierta. Significa que es muy probable que sea cierto dada la evidencia disponible. El valor p no es la probabilidad de que los datos se ajusten a una distribución gaussiana; se puede considerar como un valor que nos ayuda a interpretar la prueba estadística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Prueba de Shapiro-Wilk\n",
    "La prueba de Shapiro-Wilk evalúa una muestra de datos y cuantifica la probabilidad de que los datos se extraigan de una distribución gaussiana, llamada así por Samuel Shapiro y Martin Wilk.\n",
    "\n",
    "En la práctica, se cree que la prueba de Shapiro-Wilk es una prueba confiable de normalidad, aunque se sugiere que la prueba puede ser adecuada para muestras de datos más pequeñas, por ejemplo, miles de observaciones o menos.\n",
    "\n",
    "La función Shapiro () SciPy calculará el Shapiro-Wilk en un conjunto de datos dado. La función devuelve tanto el estadístico W calculado por la prueba como el valor p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test = st.shapiro(churners_ing)\n",
    "shapiro_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('estadístico:',round(shapiro_test[0],3))\n",
    "print('p_value:',round(shapiro_test[1],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normality test\n",
    "stat, p = st.shapiro(churners_ing)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Prueba de D'Agostino\n",
    "La prueba K² de D'Agostino calcula estadísticas de resumen a partir de los datos, a saber, curtosis y asimetría, para determinar si la distribución de datos se aparta de la distribución normal, llamada así por Ralph D'Agostino.\n",
    "\n",
    "* El sesgo es una cuantificación de cuánto se empuja una distribución hacia la izquierda o hacia la derecha, una medida de asimetría en la distribución.\n",
    "* La curtosis cuantifica qué parte de la distribución hay en la cola. Es una prueba estadística simple y de uso común para la normalidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D'Agostino and Pearson's Test\n",
    "from scipy.stats import normaltest\n",
    "\n",
    "# normality test\n",
    "stat, p = normaltest(churners_ing)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prueba de Anderson-Darling\n",
    "La prueba de Anderson-Darling es una prueba estadística que se puede utilizar para evaluar si una muestra de datos proviene de una de las muchas muestras de datos conocidas, que lleva el nombre de Theodore Anderson y Donald Darling.\n",
    "\n",
    "Se puede utilizar para comprobar si una muestra de datos es normal. La prueba es una versión modificada de una prueba estadística de bondad de ajuste no paramétrica más sofisticada llamada prueba de Kolmogorov-Smirnov.\n",
    "\n",
    "Una característica de la prueba de Anderson-Darling es que devuelve una lista de valores críticos en lugar de un solo valor p. Esto puede proporcionar la base para una interpretación más completa del resultado.\n",
    "\n",
    "La función anderson de SciPy implementa la prueba Anderson-Darling. Toma como parámetros la muestra de datos y el nombre de la distribución para probarla. De forma predeterminada, la prueba se comparará con la distribución gaussiana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anderson-Darling Test\n",
    "from scipy.stats import anderson\n",
    "\n",
    "# normality test\n",
    "result = anderson(churners_ing)\n",
    "print('Statistic: %.3f' % result.statistic)\n",
    "p = 0\n",
    "\n",
    "for i in range(len(result.critical_values)):\n",
    "    sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < result.critical_values[i]:\n",
    "        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n",
    "    else:\n",
    "        print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos interpretar los resultados si no rechazamos la hipótesis nula de que los datos son normales si el estadístico de prueba calculado es menor que el valor crítico en un nivel de significancia elegido.\n",
    "\n",
    "Podemos ver que en cada nivel de significancia, la prueba ha encontrado que los datos no siguen una distribución normal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prueba Kolmogorov-Smirnov\n",
    "\n",
    "En estadística, la prueba de Kolmogórov-Smirnov (también prueba K-S) es una prueba no paramétrica que determina la bondad de ajuste de dos distribuciones de probabilidad entre sí.\n",
    "\n",
    "La prueba de una muestra compara la distribución subyacente F (x) de una muestra con una distribución dada G (x). La prueba de dos muestras compara las distribuciones subyacentes de dos muestras independientes. Ambas pruebas son válidas solo para distribuciones continuas.\n",
    "\n",
    "Conviene tener en cuenta que la prueba Kolmogórov-Smirnov es más sensible a los valores cercanos a la mediana que a los extremos de la distribución. La prueba de Anderson-Darling proporciona igual sensibilidad con valores extremos.\n",
    "\n",
    "La hipótesis nula es que las dos distribuciones son idénticas, F (x) = G (x) para todo x; la alternativa es que no sean idénticos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para utilizar este test como prueba de normalidad, debemos antes estandarizar la data a probar:\n",
    "                                `normed_data = (data - data.mean()) / data.std()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# estandarizamos\n",
    "churners_std = (churners_ing - churners_ing.mean()) / churners_ing.std()\n",
    "\n",
    "# normality test\n",
    "stat, p = kstest(churners_std,'norm')\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se debe rechazar la hipótesis nula a favor de la hipótesis alternativa. Por lo que la distribución de x no se aproxima a la normal.\n",
    "\n",
    "Hagamos la prueba con 10 mil números aleatorios de una distribución normal estándar con esta prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "x = np.random.randn(10000)\n",
    "kstest(x, 'norm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se puede rechazar la hipótesis nula a favor de la hipótesis alternativa. Por lo que la distribución de x se aproxima a la normal.\n",
    "\n",
    "No olvidar estandarizar antes de hacer la prueba KS para normalidad para cualquier conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = 10, 5 # media y desvio estandar\n",
    "normal = st.norm(mu, sigma)\n",
    "\n",
    "x_test = normal.rvs(10000)\n",
    "\n",
    "# estandarizamos\n",
    "x_test_std = (x_test - x_test.mean()) / x_test.std()\n",
    "\n",
    "# normality test\n",
    "stat, p = kstest(x, 'norm')\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No se puede rechazar la hipótesis nula a favor de la hipótesis alternativa. Por lo que la distribución de x se aproxima a la normal.\n",
    "\n",
    "Si queremos comparar la distribución para 2 muestras independientes con KS, podemos usar `ks_2samp`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1 = np.genfromtxt(\"data/week1.csv\",  delimiter=\",\")\n",
    "week2 = np.genfromtxt(\"data/week2.csv\",  delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(week1, bins=20, alpha=0.5, color='b')\n",
    "plt.hist(week2, bins=20,alpha=0.5, color='r')\n",
    "leg = ['week1', 'week2']\n",
    "plt.legend(leg)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_2samp(week1,week2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Ho: la distribución de las 2 muestras son iguales\n",
    "* Ha: la distribución de las 2 muestras son diferentes\n",
    "\n",
    "Se debe rechazar la hipótesis nula a favor de la hipótesis alternativa. Por lo que la distribución de los datos son diferentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proceso de estandarización de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estandarización de datos es un paso muy importante en el preprocesamiento de datos. En aplicaciones prácticas, a menudo nos encontramos con un conjunto de datos que contiene una variedad de características, a menudo con diferentes distribuciones e intervalos, con diferentes niveles (dimensión), que es fácil de afectar nuestra capacitación modelo.. La estandarización de datos es eliminar los efectos de la escala, la característica y las diferencias de distribución en el modelo.\n",
    "\n",
    "Además, después de que toda la función esté estandarizada, podemos hacer carteras ponderadas para generar nuevos indicadores, y los datos en bruto a menudo no admiten que ponderemos directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Estandarización z\n",
    "\n",
    "La estandarización de la puntuación Z se estandariza en función de la normalización de la media y la desviación estándar de los datos originales, y su fórmula de transformación es:\n",
    "\n",
    "**normalización = ( x – media ) / desviación típica**\n",
    "\n",
    "Este método es adecuado para la mayoría de los tipos de datos, que es muy amplio. Desde la fórmula, podemos ver que la media se convertirá en 0 después de la transformación, y la variación y la diferencia estándar se convertirán en 1 (considerada la fórmula de la varianza), esta parte si no entiende, es posible que desee razonarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4Xz0nyzU-El"
   },
   "outputs": [],
   "source": [
    "# estandarización de datos\n",
    "score_churners = st.zscore(churners_ing) # estandarizamos los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_churners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Escalamiento MIN-MAX\n",
    "\n",
    "El método de estandarización MIN-MAX transformará linealmente los datos originales, y su fórmula de conversión es:\n",
    "\n",
    "**escalamiento = ( x – xmin ) / ( xmax – xmin )**\n",
    "\n",
    "Obviamente, cuando X es el valor máximo, se convertirá a 1; cuando X es un valor mínimo, el nuevo valor correspondiente es 0; El conjunto completo de datos se distribuye dentro del intervalo de 0 a 1, y la distribución de datos. y no cambiará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# escalado de datos\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler_churners = scaler.fit_transform(np.reshape(np.array(churners_ing),(-1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparando resultados de variables generadas\n",
    "a = pd.DataFrame(np.array(churners_ing))\n",
    "b = pd.DataFrame(score_churners)\n",
    "c = pd.DataFrame(scaler_churners)\n",
    "\n",
    "variable = pd.concat([a,b,c],axis = 1)\n",
    "variable.columns = ['churners_ing','score_churners','scaler_churners']\n",
    "variable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Il9G01tXrdx"
   },
   "outputs": [],
   "source": [
    "variable.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GO56J9ZEX1hy"
   },
   "outputs": [],
   "source": [
    "print('churners promedio:',round(variable.churners_ing.mean(),2))\n",
    "print('churners varianza:',round(variable.churners_ing.std(),4))\n",
    "\n",
    "print('score promedio:',round(variable.score_churners.mean(),2))\n",
    "print('score varianza:',round(variable.score_churners.std(),4))\n",
    "\n",
    "print('escaler promedio:',round(variable.scaler_churners.mean(),2))\n",
    "print('escaler varianza:',round(variable.scaler_churners.std(),4))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "[14082020]_Estadistica_Inferencial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
