{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Análisis de Regresión y Regularizaciones</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libs\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from math import sqrt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "\n",
    "## Modelos de regresion\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import statsmodels.api as sm           \n",
    "import statsmodels.formula.api as smf  \n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Metricas de evaluación\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#separar train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# estadística y matemáticas\n",
    "#import pandas as pd\n",
    "import scipy.stats as scy\n",
    "from scipy.stats import kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datos del Negocio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('data/demanda_data1.csv',sep=',',parse_dates = ['tiempo'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Análisis Exploratorio de los Datos (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograma del Target\n",
    "sns.set()\n",
    "plt.figure()\n",
    "sns.distplot(dataset[\"UnidadesDemandas\"] , label=\"UnidadesDemandas\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped boxplot\n",
    "plt.figure()\n",
    "sns.boxplot(x='UnidadesDemandas', data=dataset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correlaciones lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=2)\n",
    "corr_matrix = dataset.corr()\n",
    "plt.figure(figsize=(16, 10))\n",
    "ax = sns.heatmap(corr_matrix,annot=True, fmt=\".1f\",cmap=\"YlGnBu\") \n",
    "ax.set_ylim(sorted(ax.get_xlim(), reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribución de las variables cuantitativas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(14, 12))\n",
    "axes = axes.flat\n",
    "columnas_object = dataset.select_dtypes(include=['float']).columns\n",
    "\n",
    "for i, colum in enumerate(columnas_object):\n",
    "    dataset[colum].value_counts().plot.hist(ax = axes[i], bins=30)\n",
    "    axes[i].set_title(colum, fontsize = 12, fontweight = \"bold\")\n",
    "    axes[i].tick_params(labelsize = 4)\n",
    "    axes[i].set_xlabel(\"\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "fig.suptitle('Distribución variables numericas',\n",
    "             fontsize = 18, fontweight = \"bold\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gráfico para cada variable cualitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in ['Marca']:\n",
    "    var = dataset.groupby(feature)[feature].count().sort_values(ascending = False)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,1,1)\n",
    "    ax1.set_xlabel(feature)\n",
    "    ax1.set_ylabel('Cantidad')\n",
    "    ax1.set_title(feature)\n",
    "    var.plot(kind='bar')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocesing de la data cualitativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.get_dummies(dataset, columns=['Marca']) \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Modelos de Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selección de muestras de entrenamiento y validación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data[dataset.tiempo < '2014-01-01'].drop(['UnidadesDemandas','tiempo'], axis=1)\n",
    "X_test = data[dataset.tiempo >= '2014-01-01'].drop(['UnidadesDemandas','tiempo'], axis=1)\n",
    "y_train = data[dataset.tiempo < '2014-01-01'][['UnidadesDemandas']]\n",
    "y_test= data[dataset.tiempo >= '2014-01-01'][['UnidadesDemandas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segundo método con split aleatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = dataset.drop(['UnidadesDemandas','tiempo'], axis=1)\n",
    "#y = dataset[['UnidadesDemandas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.Modelo de Regresión Lineal Múltiple (usando librería statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_R = sm.OLS(y_train,X_train).fit()\n",
    "M_R.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hallazgos:\n",
    "* El p valor de la prueba ANOVA nos valida que el modelo de regresión es válido.\n",
    "* De encontrar variables que no cumplen la hipótesis individual (prueba t) hay que quitarlos y volver a estimar el modelo de regresión.\n",
    "* Variables como `CantOfertas` y `TieneUbicacionEspecifica` no cumplen con la validez individual, deberían excluirse del modelo, veremos como se desempeñan estas variables con las técnicas regularizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción del test\n",
    "y_pred = M_R.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo\n",
    "print('Mean Absolute Error:',      metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:',       metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:',  np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "rmse_ols = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "print(f\"El error (rmse) de test es: {rmse_ols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las predicciones del modelo final se alejan en promedio 42.321 unidades del valor real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 del modelo \n",
    "r2_ols = np.sqrt(r2_score(y_test,y_pred))\n",
    "print(\"\")\n",
    "print(f\"El R2 de test es: {r2_ols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisis Residual y Supuestos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valores predecidos vs valores reales (Linealidad)\n",
    "plt.scatter(y_test, y_pred, edgecolors=(0, 0, 0), alpha = 0.4)\n",
    "plt.plot([y_test.min(), y_test.max()], \n",
    "         [y_test.min(), y_test.max()],'k--', color = 'black', lw=2)\n",
    "plt.title('Valor predicho vs valor real', fontsize = 20, fontweight = \"bold\")\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Predicción')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prueba de normalidad \n",
    "from scipy.stats import shapiro\n",
    "stat, p = shapiro(y_pred)\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalidad\n",
    "p = sns.distplot(y_pred,kde=True)\n",
    "p = plt.title('Normalidad de los Residuales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La multicolinealidad en el análisis de regresión ocurre cuando dos o más variables explicativas están altamente correlacionadas entre sí, de manera que no brindan información única o independiente en el modelo de regresión. Si el grado de correlación entre variables es lo suficientemente alto, puede causar problemas al ajustar e interpretar el modelo de regresión.\n",
    "\n",
    "Una forma de detectar la multicolinealidad es mediante el uso de una métrica conocida como factor de inflación de la varianza (VIF) , que mide la correlación y la fuerza de la correlación entre las variables explicativas en un modelo de regresión ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#calcular VIF para cada variable explicativa\n",
    "vif = pd.DataFrame ()\n",
    "vif ['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range (X_train.shape[1])]\n",
    "vif ['variable'] = X_train.columns\n",
    "\n",
    "#ver VIF para cada variable explicativa \n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El valor de VIF comienza en 1 y no tiene límite superior. Una regla general para interpretar los VIF es la siguiente:\n",
    "\n",
    "* Un valor de 1 indica que no hay correlación entre una variable explicativa dada y cualquier otra variable explicativa en el modelo.\n",
    "* Un valor entre 1 y 5 indica una correlación moderada entre una variable explicativa dada y otras variables explicativas en el modelo, pero esto a menudo no es lo suficientemente grave como para requerir atención.\n",
    "* Un valor mayor que 5 indica una correlación potencialmente severa entre una variable explicativa dada y otras variables explicativas en el modelo. En este caso, las estimaciones de los coeficientes y los valores p en el resultado de la regresión probablemente no sean confiables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homocedastidiad es decir errores uniformes en sus residuos\n",
    "plt.scatter(list(range(len(y_test))), \n",
    "           y_test.index.values - np.array(y_pred), edgecolors=(0, 0, 0), alpha = 0.4)\n",
    "plt.axhline(y = 3400, linestyle = '--', color = 'black', lw=2)\n",
    "plt.title('Residuos del modelo', fontsize = 20, fontweight = \"bold\")\n",
    "plt.xlabel('id')\n",
    "plt.ylabel('Residuo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos crear nuestra metodología de medición de regresores!\n",
    "\n",
    "df_test = pd.concat([X_test,y_test],axis=1)\n",
    "\n",
    "df_test['y_pred_lineal'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Modelos lineales con Regularización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import ElasticNetCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Regularización Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación y entrenamiento del modelo (con búsqueda por CV del valor óptimo alpha)\n",
    "# Por defecto RidgeCV utiliza el mean squared error\n",
    "modelo_RG = RidgeCV(\n",
    "            alphas          = [1000,800,600,400,200,100,80,60,30,15,10,9,8,7,6,5,1,0.1,0.01,0.005,0.0001,0.00005],\n",
    "            fit_intercept   = True,\n",
    "            normalize       = True,\n",
    "            store_cv_values = True\n",
    "         )\n",
    "\n",
    "_ = modelo_RG.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando se utiliza regularización, es útil evaluar cómo se aproximan a cero los coeficientes a medida que se incrementa el valor de alpha así como la evolución del error de la optimización en función del alpha empleado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_RG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolución de los coeficientes en función de alpha\n",
    "alphas = modelo_RG.alphas\n",
    "coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    modelo_temp = Ridge(alpha=alpha, fit_intercept=True, normalize=True)\n",
    "    modelo_temp.fit(X_train, y_train)\n",
    "    coefs.append(modelo_temp.coef_.flatten())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('coeficientes')\n",
    "ax.set_title('Coeficientes del modelo en función de la regularización');\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A medida que aumenta el valor de alpha, la regularización es mayor y el valor de los coeficientes se reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el valor de alpha es:\n",
    "modelo_RG.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor modelo alpha óptimo + 1sd\n",
    "modelo_RG = Ridge(alpha=0.1, normalize=True)\n",
    "modelo_RG.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes del modelo\n",
    "# ==============================================================================\n",
    "df_coeficientes = pd.DataFrame(\n",
    "                        {'predictor': X_train.columns,\n",
    "                         'coef': modelo_RG.coef_.flatten()})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
    "plt.xticks(rotation=90, ha='right', size=10)\n",
    "ax.set_xlabel('variable')\n",
    "ax.set_ylabel('coeficientes')\n",
    "ax.set_title('Coeficientes del modelo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos los coeficientes\n",
    "print(pd.Series(modelo_RG.coef_.ravel(), index = X_train.columns).sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones del test\n",
    "predicciones = modelo_RG.predict(X=X_test)\n",
    "predicciones = predicciones.flatten()\n",
    "predicciones[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo \n",
    "rmse_ridge = np.sqrt(metrics.mean_squared_error(y_test, predicciones))\n",
    "print(\"\")\n",
    "print(f\"El ECM de test es: {rmse_ridge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las predicciones del modelo final se alejan en promedio 42.313 unidades del valor real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Valores altos de alpha producen modelo que tienden a ajustar los pesos a cero. Es decir aumenta el sesgo del modelo\n",
    "\n",
    "* Valores bajos de alpha lo que produce es que el modelo se comporta como una regresión lineal sin regularización\n",
    "\n",
    "En ese sentido es muy importante elegir el valor optimo de alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 del modelo \n",
    "r2_ridge = np.sqrt(r2_score(y_test,predicciones))\n",
    "print(\"\")\n",
    "print(f\"El error (rmse) de test es: {r2_ridge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Regularización Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación y entrenamiento del modelo (con búsqueda por CV del valor óptimo alpha)\n",
    "# Por defecto LassoCV utiliza el mean squared error\n",
    "modelo_LS = LassoCV(\n",
    "            alphas          = [1000,800,600,400,200,100,80,60,30,15,10,9,8,7,6,5,1,0.1,0.01,0.001],\n",
    "            normalize       = True,\n",
    "            cv              = 10\n",
    "         )\n",
    "_ = modelo_LS.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evolución de los coeficientes en función de alpha\n",
    "alphas = modelo_LS.alphas_\n",
    "coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    modelo_temp = Lasso(alpha=alpha, fit_intercept=False, normalize=True)\n",
    "    modelo_temp.fit(X_train, y_train)\n",
    "    coefs.append(modelo_temp.coef_.flatten())\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim([-15,None])\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('coeficientes')\n",
    "ax.set_title('Coeficientes del modelo en función de la regularización');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A medida que aumenta el valor de alpha, la regularización es mayor y más predictores quedan excluidos (su coeficiente es 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de predictores incluidos (coeficiente !=0) en función de alpha\n",
    "alphas = modelo_LS.alphas_\n",
    "n_predictores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    modelo_temp = Lasso(alpha=alpha, fit_intercept=True, normalize=True)\n",
    "    modelo_temp.fit(X_train, y_train)\n",
    "    coef_no_cero = np.sum(modelo_temp.coef_.flatten() != 0)\n",
    "    n_predictores.append(coef_no_cero)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(alphas, n_predictores)\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylim([-15,None])\n",
    "ax.set_xlabel('alpha')\n",
    "ax.set_ylabel('nº predictores')\n",
    "ax.set_title('Predictores incluidos en función de la regularización');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# el valor de alpha es:\n",
    "modelo_LS.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor modelo alpha óptimo + 1sd\n",
    "modelo_LS = Lasso(alpha=0.1, normalize=True)\n",
    "modelo_LS.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes del modelo\n",
    "df_coeficientes = pd.DataFrame(\n",
    "                        {'predictor': X_train.columns,\n",
    "                         'coef': modelo_LS.coef_.flatten()}\n",
    "                  )\n",
    "\n",
    "# Predictores incluidos en el modelo (coeficiente != 0)\n",
    "df_coeficientes[df_coeficientes.coef != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 3.84))\n",
    "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
    "plt.xticks(rotation=90, ha='right', size=10)\n",
    "ax.set_xlabel('variable')\n",
    "ax.set_ylabel('coeficientes')\n",
    "ax.set_title('Coeficientes del modelo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos los coeficientes\n",
    "print(pd.Series(modelo_LS.coef_, index = X_train.columns).sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones test\n",
    "predicciones = modelo_LS.predict(X=X_test)\n",
    "predicciones = predicciones.flatten()\n",
    "predicciones[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo \n",
    "rmse_lasso = np.sqrt(metrics.mean_squared_error(y_test, predicciones))\n",
    "print(\"\")\n",
    "print(f\"El error (rmse) de test es: {rmse_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las predicciones del modelo final se alejan en promedio 44.394 unidades del valor real, utilizando solo 8 variables del total de predictores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 del modelo \n",
    "r2_lasso = np.sqrt(r2_score(y_test,predicciones))\n",
    "print(\"\")\n",
    "print(f\"El R2 de test es: {r2_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Regularización Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación y entrenamiento del modelo (con búsqueda por CV del valor óptimo alpha)\n",
    "# Por defecto ElasticNetCV utiliza el mean squared error\n",
    "modelo_EL = ElasticNetCV(\n",
    "            l1_ratio        = [0, 0.1, 0.5, 0.7, 0.9, 0.95, 0.99],\n",
    "            alphas          = [1000,800,600,400,200,100,80,60,30,15,10,9,8,7,6,5,1,0.1,0.01,0.001],\n",
    "            normalize       = True,\n",
    "            cv              = 10\n",
    "         )\n",
    "_ = modelo_EL.fit(X = X_train, y = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_EL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_EL.l1_ratio_  #ratio de ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_EL.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coeficientes del modelo\n",
    "df_coeficientes = pd.DataFrame(\n",
    "                        {'predictor': X_train.columns,\n",
    "                         'coef': modelo_EL.coef_.flatten()}\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(11, 3.84))\n",
    "ax.stem(df_coeficientes.predictor, df_coeficientes.coef, markerfmt=' ')\n",
    "plt.xticks(rotation=90, ha='right', size=10)\n",
    "ax.set_xlabel('variable')\n",
    "ax.set_ylabel('coeficientes')\n",
    "ax.set_title('Coeficientes del modelo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pintamos los coeficientes\n",
    "print(pd.Series(modelo_EL.coef_, index = X_train.columns).sort_values(ascending = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones test\n",
    "predicciones = modelo_EL.predict(X=X_test)\n",
    "predicciones = predicciones.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error de test del modelo \n",
    "rmse_elastic = np.sqrt(mean_squared_error(y_test,predicciones))\n",
    "print(\"\")\n",
    "print(f\"El error (rmse) de test es: {rmse_elastic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las predicciones del modelo final se alejan en promedio 42.088 unidades del valor real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2 del modelo \n",
    "r2_elastic = np.sqrt(r2_score(y_test,predicciones))\n",
    "print(\"\")\n",
    "print(f\"El R2 de test es: {r2_elastic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARACION DEL MEJOR MODELO EN BASE AL RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos los 4 modelos\n",
    "df_comparacion1 = pd.DataFrame({\n",
    "                    'modelo': ['OLS', 'Ridge', 'Lasso', 'Elastic-net'],\n",
    "                    'test rmse': [rmse_ols, rmse_ridge, rmse_lasso, rmse_elastic]\n",
    "                 })\n",
    "ax = df_comparacion1.set_index('modelo').plot(kind=\"barh\",title=\"Comparación de Modelos con RMSE TEST\")\n",
    "ax.set_ylabel(\"RMSE\", fontsize=\"large\")\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparacion1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparamos los 4 modelos\n",
    "df_comparacion2 = pd.DataFrame({\n",
    "                    'modelo': ['OLS', 'Ridge', 'Lasso', 'Elastic-net'],\n",
    "                    'test rmse': [r2_ols, r2_ridge, r2_lasso, r2_elastic]\n",
    "                 })\n",
    "ax = df_comparacion2.set_index('modelo').plot(kind=\"barh\",title=\"Comparación de Modelos con R2 TEST\")\n",
    "ax.set_ylabel(\"R2\", fontsize=\"large\")\n",
    "ax.get_legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparacion2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusión: Ridge tine mas explicativas por eso r2 mayor, pero Lasso nos da una regresión con las mejores variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
